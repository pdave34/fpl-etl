{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"FPL ETL Documentation","text":"<p>Welcome to the FPL ETL project documentation. This project provides a lightweight ETL pipeline that extracts data from external APIs (e.g., Fantasy Premier League), cleans it, and passes PyArrow tables for downstream analytics.</p>"},{"location":"#overview","title":"Overview","text":"<ul> <li>Extractors: Pull data from APIs (e.g., Fantasy Premier League) and return Polars DataFrames.</li> <li>Utils: Helper functions for data cleaning and ID generation.</li> <li>Loaders: Write data to Parquet or generate Oracle DDL.</li> <li>Pipeline: Orchestrates the extract\u2011transform\u2011load flow.</li> </ul> <p>See the navigation menu on the left for detailed module documentation.</p>"},{"location":"src/etl/extractors/","title":"Extractors Module","text":"<p>This module contains classes for extracting data from external APIs and converting them into Polars <code>DataFrame</code>s.</p>"},{"location":"src/etl/extractors/#classes","title":"Classes","text":"<ul> <li><code>APIExtractor</code> \u2013 Base class providing common extraction logic.</li> <li><code>FPL</code> \u2013 Subclass for the Fantasy Premier League API.</li> </ul>"},{"location":"src/etl/extractors/#usage","title":"Usage","text":"<pre><code>from etl.extractors import FPL\nextractor = FPL()\nfor name, df in extractor.generate():\n    # Process DataFrame\n    pass\n</code></pre>"},{"location":"src/etl/extractors/#source","title":"Source","text":"<p>ETL extractors for reading data from APIs.</p> <p>This module defines an <code>APIMetrics</code> dataclass for recording request and response timing information (timestamps and elapsed time in milliseconds), and an <code>APIExtractor</code> that returns both the parsed JSON payload and the associated metrics.</p>"},{"location":"src/etl/extractors/#etl.extractors.APIExtractor","title":"<code>APIExtractor</code>","text":"<p>Extractor for fetching data from APIs and recording metrics.</p> <p>The <code>fetch_data</code> method returns a tuple of the parsed JSON response and an <code>APIMetrics</code> instance with timing and status information.</p> Source code in <code>src/etl/extractors.py</code> <pre><code>class APIExtractor:\n    \"\"\"Extractor for fetching data from APIs and recording metrics.\n\n    The `fetch_data` method returns a tuple of the parsed JSON response and\n    an `APIMetrics` instance with timing and status information.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the BaseLoader with logging configuration.\"\"\"\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        )\n        self.logger: Logger = logging.getLogger(name=__name__)\n\n    def stamp(self) -&gt; dict:\n        return {\n            \"request_id\": str(uuid.uuid4()),\n            \"request_time\": datetime.now(timezone.utc).isoformat(sep=\" \", timespec=\"milliseconds\"),\n        }\n\n    def get(self, url: str, *args, **kwargs) -&gt; dict:\n        stamp: dict = self.stamp()\n        response: Response = requests.get(url, *args, **kwargs)\n        stamp[\"response\"] = response\n        stamp[\"response_code\"] = response.status_code\n        stamp[\"response_elapsed_ms\"] = response.elapsed.total_seconds() * 1000\n        return stamp\n\n    def parse(self, stamp: dict, key: Optional[str]) -&gt; DataFrame:\n        if stamp[\"response_code\"] == 200:\n            if key is None:\n                stamp[\"data\"] = stamp[\"response\"].json()\n            else:\n                stamp[key] = stamp[\"response\"].json().get(key)\n        else:\n            stamp[\"data\"] = None\n        del stamp[\"response\"]\n        # Build DataFrame based on response success and key presence\n        if stamp[\"response_code\"] == 200 and key is not None:\n            data = stamp.get(key) or []\n            df = pl.DataFrame(data)\n        else:\n            # Empty DataFrame for failure or missing key, but include metric columns (no rows)\n            df = pl.DataFrame(\n                {\n                    \"request_id\": [],\n                    \"request_time\": [],\n                    \"response_code\": [],\n                    \"response_elapsed_ms\": [],\n                }\n            )\n        df = clean_df(df)\n        # Enrich with metric columns; works for empty DataFrames as well\n        df = (\n            df.with_columns(pl.lit(stamp[\"request_id\"]).alias(\"request_id\"))\n            .with_columns(pl.lit(stamp[\"request_time\"]).alias(\"request_time\"))\n            .with_columns(pl.lit(stamp[\"response_code\"]).alias(\"response_code\"))\n            .with_columns(pl.lit(stamp[\"response_elapsed_ms\"]).alias(\"response_elapsed_ms\"))\n        )\n        return df\n\n    def save(self, df: DataFrame, filename: str) -&gt; None:\n        df.write_parquet(file=filename)\n</code></pre>"},{"location":"src/etl/extractors/#etl.extractors.APIExtractor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the BaseLoader with logging configuration.</p> Source code in <code>src/etl/extractors.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the BaseLoader with logging configuration.\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    )\n    self.logger: Logger = logging.getLogger(name=__name__)\n</code></pre>"},{"location":"src/etl/loaders/","title":"Loaders Module","text":"<p>This module handles writing transformed data to storage formats such as Parquet files or generating Oracle DDL statements.</p>"},{"location":"src/etl/loaders/#key-functions","title":"Key Functions","text":"<ul> <li><code>write_parquet(df, filename)</code> \u2013 Write a Polars <code>DataFrame</code> to a Parquet file.</li> <li><code>generate_oracle_ddl(df, table_name)</code> \u2013 Produce DDL for Oracle based on the DataFrame schema.</li> </ul>"},{"location":"src/etl/loaders/#usage","title":"Usage","text":"<pre><code>from etl.loaders import write_parquet\nwrite_parquet(df, \"data/output.parquet\")\n</code></pre>"},{"location":"src/etl/loaders/#source","title":"Source","text":"<p>ETL loaders for reading and inserting data into various data sources.</p> <p>This module provides classes for loading data from files into different backends, including Oracle Database. It supports CSV, Excel, and JSON formats.</p>"},{"location":"src/etl/loaders/#etl.loaders.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>Base class for data loaders with common functionality.</p> <p>Provides methods for reading data from various file formats (CSV, Excel, JSON) and logging capabilities.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>class BaseLoader:\n    \"\"\"Base class for data loaders with common functionality.\n\n    Provides methods for reading data from various file formats (CSV, Excel, JSON)\n    and logging capabilities.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the BaseLoader with logging configuration.\"\"\"\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        )\n        self.logger = logging.getLogger(__name__)\n\n    def read_data(self, source: Path) -&gt; pd.DataFrame:\n        \"\"\"Read data from a file into a pandas DataFrame.\n\n        Args:\n            source: Path to the file. Supports CSV, Excel (.xlsx), and JSON formats.\n\n        Returns:\n            A pandas DataFrame containing the file data.\n\n        Raises:\n            ValueError: If the file format is not supported.\n        \"\"\"\n        self.logger.info(f\"Reading data from {source}\")\n        if source.suffix.lower() == \".csv\":\n            df = pd.read_csv(source)\n        elif source.suffix.lower() == \".xlsx\":\n            df = pd.read_excel(source)\n        elif source.suffix.lower() == \".json\":\n            data = json.load(open(source, \"r\"))\n            df = pd.DataFrame(data)\n        else:\n            self.logger.error(f\"Unsupported file format: {source.suffix}\")\n            raise ValueError(f\"Unsupported file format: {source.suffix}\")\n        self.logger.info(f\"Data read successfully with shape {df.shape}\")\n        return pa.table(df)\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.BaseLoader.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the BaseLoader with logging configuration.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the BaseLoader with logging configuration.\"\"\"\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    )\n    self.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.BaseLoader.read_data","title":"<code>read_data(source)</code>","text":"<p>Read data from a file into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path</code> <p>Path to the file. Supports CSV, Excel (.xlsx), and JSON formats.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame containing the file data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def read_data(self, source: Path) -&gt; pd.DataFrame:\n    \"\"\"Read data from a file into a pandas DataFrame.\n\n    Args:\n        source: Path to the file. Supports CSV, Excel (.xlsx), and JSON formats.\n\n    Returns:\n        A pandas DataFrame containing the file data.\n\n    Raises:\n        ValueError: If the file format is not supported.\n    \"\"\"\n    self.logger.info(f\"Reading data from {source}\")\n    if source.suffix.lower() == \".csv\":\n        df = pd.read_csv(source)\n    elif source.suffix.lower() == \".xlsx\":\n        df = pd.read_excel(source)\n    elif source.suffix.lower() == \".json\":\n        data = json.load(open(source, \"r\"))\n        df = pd.DataFrame(data)\n    else:\n        self.logger.error(f\"Unsupported file format: {source.suffix}\")\n        raise ValueError(f\"Unsupported file format: {source.suffix}\")\n    self.logger.info(f\"Data read successfully with shape {df.shape}\")\n    return pa.table(df)\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader","title":"<code>OracleDBLoader</code>","text":"<p>               Bases: <code>BaseLoader</code></p> <p>Loader for reading data from files and inserting into Oracle Database.</p> <p>Manages connections to Oracle Database using connection pooling and provides methods for creating tables, inserting data, and querying metadata.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>class OracleDBLoader(BaseLoader):\n    \"\"\"Loader for reading data from files and inserting into Oracle Database.\n\n    Manages connections to Oracle Database using connection pooling and provides\n    methods for creating tables, inserting data, and querying metadata.\n    \"\"\"\n\n    def __init__(\n        self,\n        dsn: Optional[str] = None,\n        user: Optional[str] = None,\n        password: Optional[str] = None,\n        min_pool: int = 1,\n        max_pool: int = 1,\n        increment_pool: int = 0,\n        timeout: int = 60,\n    ):\n        \"\"\"Initialize OracleDBLoader with connection pool.\n\n        Args:\n            dsn: Oracle database service name. Defaults to env variable \"ORACLE_DSN\".\n            user: Database username. Defaults to env variable \"ORACLE_USR\".\n            password: Database password. Defaults to env variable \"ORACLE_PWD\".\n            min_pool: Minimum pool size. Defaults to 1.\n            max_pool: Maximum pool size. Defaults to 1.\n            increment_pool: Pool increment size. Defaults to 0.\n            timeout: Connection timeout in seconds. Defaults to 60.\n\n        Raises:\n            DatabaseError: If connection pool creation fails.\n        \"\"\"\n        super().__init__()\n        self.dsn = dsn or os.getenv(\"ORACLE_DSN\")\n        self.user = user or os.getenv(\"ORACLE_USR\")\n        self.password = password or os.getenv(\"ORACLE_PWD\")\n        self.pool_alias = utils.id_generator()\n        odb.defaults.config_dir = (\n            \"/Users/astropd/Projects/oracle/oracle-19c/oradata/dbconfig/D7WOB1D1\"\n        )\n        odb.create_pool(\n            user=self.user,\n            password=self.password,\n            dsn=self.dsn,\n            min=min_pool,\n            max=max_pool,\n            increment=increment_pool,\n            pool_alias=self.pool_alias,\n            timeout=timeout,\n        )\n        self.test_connection()\n\n    def test_connection(self) -&gt; bool:\n        \"\"\"Test the connection to the Oracle database.\n\n        Returns:\n            True if connection is healthy, False otherwise.\n        \"\"\"\n        self.logger.info(f\"Testing connection to Oracle DB at {self.dsn}\")\n        healthy = False\n        try:\n            with odb.connect(pool_alias=self.pool_alias) as conn:\n                healthy = conn.is_healthy()\n                self.logger.info(f\"Connection healthy: {healthy}\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Connection failed: {e}\")\n            healthy = False\n        return healthy\n\n    def insert_data(self, data: pa.Table, table_name: str) -&gt; None:\n        \"\"\"Insert pyarrow Table data into an Oracle table using parameterized queries.\n\n        Args:\n            data: pyarrow Table containing the data to insert.\n            table_name: Name of the target table.\n        \"\"\"\n        self.logger.info(f\"Inserting data into table {table_name}\")\n        timing = []\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cols = \", \".join([f'\"{c.upper()}\"' for c in data.column_names])\n            placeholders = \",\".join([f\":{i + 1}\" for i in range(len(data.column_names))])\n            sql = f\"INSERT INTO {table_name} ({cols}) VALUES ({placeholders})\"\n            cursor = conn.cursor()\n            timing.append(time.perf_counter())\n            cursor.executemany(sql, data)\n            conn.commit()\n            timing.append(time.perf_counter())\n            self.logger.info(\n                f\"Inserted {cursor.rowcount} rows into {table_name} in \\\n                    {timing[-1] - timing[0]:.2f} seconds\"\n            )\n            self.logger.info(f\"Executed SQL: {sql}\")\n\n    def insert_full_data(self, table: pa.Table, table_name: str) -&gt; None:\n        \"\"\"Insert DataFrame data using Oracle direct path load (high performance).\n\n        Args:\n            table: pyarrow Table containing the data to insert.\n            table_name: Name of the target table.\n        \"\"\"\n        self.logger.info(f\"Inserting full data into table {table_name}\")\n        timing = []\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            timing.append(time.perf_counter())\n            conn.direct_path_load(\n                schema_name=self.user,\n                table_name=table_name,\n                column_names=table.column_names,\n                data=table,\n            )\n            conn.commit()\n            timing.append(time.perf_counter())\n            self.logger.info(\n                f\"Full data inserted into {table_name} in {timing[-1] - timing[0]:.2f} seconds\"\n            )\n\n    def truncate_table(self, table_name: str) -&gt; None:\n        \"\"\"Truncate (remove all data from) an Oracle table.\n\n        Args:\n            table_name: Name of the table to truncate.\n\n        Raises:\n            DatabaseError: If truncation fails.\n        \"\"\"\n        self.logger.info(f\"Truncating table {table_name}\")\n        try:\n            with odb.connect(pool_alias=self.pool_alias) as conn:\n                cursor = conn.cursor()\n                cursor.execute(f\"TRUNCATE TABLE {table_name}\")\n                conn.commit()\n                self.logger.info(f\"Table {table_name} truncated successfully\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Truncation failed: {e}\")\n            raise\n\n    def check_table_exists(self, table_name: str) -&gt; bool:\n        \"\"\"Check if a table exists in the database.\n\n        Args:\n            table_name: Name of the table to check.\n\n        Returns:\n            True if the table exists, False otherwise.\n\n        Raises:\n            DatabaseError: If the check fails.\n        \"\"\"\n        self.logger.info(f\"Checking if table {table_name} exists\")\n        exists = False\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\n                \"SELECT COUNT(*) FROM all_tables WHERE table_name \\\n                    = :tbl_name AND owner = :owner\",\n                tbl_name=table_name.upper(),\n                owner=self.user.upper(),\n            )\n            count = cursor.fetchone()[0]\n            exists = count &gt; 0\n            self.logger.info(f\"Table {table_name} exists: {exists}\")\n        return exists\n\n    def execute_query(self, query: str) -&gt; odb.DataFrame:\n        \"\"\"Execute a SQL query and return the results as a DataFrame.\n\n        Args:\n            query: The SQL query to execute.\n\n        Returns:\n            An Oracle DataFrame containing the query results.\n\n        Raises:\n            DatabaseError: If the query fails.\n        \"\"\"\n        self.logger.info(f\"Executing query: {query}\")\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            df: odb.DataFrame = conn.fetch_df_all(query)\n            # self.logger.info(f\"Query executed successfully with shape {df.shape}\")\n            self.logger.info(f\"Query returned {df.num_rows()} rows\")\n            return df\n\n    def count_rows(self, table_name: str) -&gt; int:\n        \"\"\"Get the number of rows in a table.\n\n        Args:\n            table_name: Name of the table.\n\n        Returns:\n            The number of rows in the table.\n\n        Raises:\n            DatabaseError: If the query fails.\n        \"\"\"\n        self.logger.info(f\"Getting row count for table {table_name}\")\n        row_count = 0\n        try:\n            with odb.connect(pool_alias=self.pool_alias) as conn:\n                cursor: odb.Cursor = conn.cursor()\n                cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n                row_count: int = cursor.fetchone()[0]\n                self.logger.info(f\"Table {table_name} has {row_count} rows\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Row count retrieval failed: {e}\")\n            raise\n        return row_count\n\n    def drop_table(self, table_name: str) -&gt; None:\n        \"\"\"Drop (delete) a table from the database.\n\n        Args:\n            table_name: Name of the table to drop.\n\n        Raises:\n            DatabaseError: If the drop operation fails.\n        \"\"\"\n        self.logger.info(f\"Dropping table {table_name}\")\n        try:\n            with odb.connect(pool_alias=self.pool_alias) as conn:\n                cursor: odb.Cursor = conn.cursor()\n                cursor.execute(f\"DROP TABLE {table_name} PURGE\")\n                conn.commit()\n                self.logger.info(f\"Table {table_name} dropped successfully\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Dropping table failed: {e}\")\n            raise\n\n    def create_table(self, table_name: str, table: pa.Table) -&gt; None:\n        \"\"\"Create a new table in the database based on DataFrame structure.\n\n        Args:\n            table_name: Name of the table to create.\n            df: DataFrame whose structure defines the table columns and types.\n\n        Raises:\n            DatabaseError: If table creation fails.\n        \"\"\"\n        ddl: str = self.generate_oracle_ddl(table, table_name)\n        self.logger.info(f\"Generated DDL: {ddl}\")\n        try:\n            with odb.connect(pool_alias=self.pool_alias) as conn:\n                cursor: odb.Cursor = conn.cursor()\n                cursor.execute(ddl)\n                conn.commit()\n                self.logger.info(f\"Table {table_name} created successfully\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Creating table failed: {e}\")\n            raise\n\n    def generate_oracle_ddl(self, table: pa.Table, table_name: str) -&gt; str:\n        \"\"\"Generate Oracle DDL CREATE TABLE statement from a pyarrow Table.\n\n        Args:\n            table: pyarrow Table whose schema defines the table columns.\n            table_name: Name of the table to create.\n\n        Returns:\n            A SQL CREATE TABLE statement string.\n        \"\"\"\n        column_defs = []\n\n        def _field_to_oracle_type(field: pa.Field, column=None):\n            t: pa.DataType = field.type\n            # Decimal -&gt; NUMBER(precision,scale)\n            if pa.types.is_decimal(t):\n                try:\n                    return f\"NUMBER({t.precision},{t.scale})\"\n                except Exception:\n                    return \"NUMBER\"\n\n            # Integers -&gt; NUMBER\n            if pa.types.is_integer(t):\n                return \"NUMBER\"\n\n            # Floats -&gt; BINARY_DOUBLE / BINARY_FLOAT\n            if pa.types.is_float64(t):\n                return \"BINARY_DOUBLE\"\n            if pa.types.is_float32(t):\n                return \"BINARY_FLOAT\"\n\n            # Booleans -&gt; NUMBER(1)\n            if pa.types.is_boolean(t):\n                return \"NUMBER(1)\"\n\n            # Binary types -&gt; BLOB\n            if pa.types.is_large_binary(t) or pa.types.is_binary(t):\n                return \"BLOB\"\n\n            # Timestamp -&gt; TIMESTAMP (with TZ if present)\n            if pa.types.is_timestamp(t):\n                tz = getattr(t, \"tz\", None)\n                if tz:\n                    return \"TIMESTAMP WITH TIME ZONE\"\n                return \"TIMESTAMP\"\n\n            # Strings -&gt; VARCHAR2 with a safe length or CLOB for large values\n            if pa.types.is_large_string(t):\n                return \"CLOB\"\n            if pa.types.is_string(t):\n                # try to infer max length from the column values if provided\n                max_len = 0\n                if column is not None:\n                    try:\n                        max_len: int = max(\n                            (len(x) for x in column.to_pylist() if x is not None), default=0\n                        )\n                    except Exception:\n                        max_len = 0\n                # conservative byte multiplier for multibyte characters\n                length = max(255, int(max_len * 3))\n                if length &lt;= 4000:\n                    return f\"VARCHAR2({min(length, 4000)})\"\n                return \"CLOB\"\n\n            # Lists or structs representing vectors -&gt; BLOB (DB_TYPE_VECTOR)\n            if pa.types.is_list(t) or pa.types.is_struct(t):\n                return \"BLOB\"\n\n            # Fallback to CLOB for other types\n            return \"CLOB\"\n\n        # Map each field in the schema to an Oracle type\n        for i, field in enumerate(table.schema):\n            col_name = field.name\n            # provide the column chunk for possible length inference\n            column = None\n            try:\n                column = table.column(i)\n            except Exception:\n                column = None\n\n            oracle_type = _field_to_oracle_type(field, column)\n            column_defs.append(f'    \"{col_name.upper()}\" {oracle_type}')\n\n        ddl = f'CREATE TABLE \"{table_name.upper()}\" (\\n' + \",\\n\".join(column_defs) + \"\\n)\"\n        return ddl\n\n    def rebuild_table(self, table_name: str, data: pa.Table):\n        if self.check_table_exists(table_name):\n            self.drop_table(table_name)\n        self.create_table(table_name, data)\n        self.insert_data(data, table_name=table_name)\n\n    def reload_table(self, table_name: str, data: pa.Table):\n        if self.check_table_exists(table_name):\n            self.truncate_table(table_name)\n        self.insert_data(data, table_name=table_name)\n\n    def upsert_data(self, data: pa.Table, table_name: str, key_columns: list[str]) -&gt; None:\n        \"\"\"Upsert pyarrow Table data into an Oracle table using MERGE statement.\n\n        Args:\n            data: pyarrow Table containing the data to upsert.\n            table_name: Name of the target table.\n            key_columns: List of column names that form the primary key for upsert.\n        \"\"\"\n        self.logger.info(f\"Upserting data into table {table_name}\")\n        timing = []\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cols = [f'\"{c.upper()}\"' for c in data.column_names]\n            insert_cols = \", \".join(cols)\n            insert_values = \",\".join([f\":{i + 1}\" for i in range(len(data.column_names))])\n            update_set = \", \".join(\n                [\n                    f\"{col} = :{i + 1}\"\n                    for i, col in enumerate(cols)\n                    if col.strip('\"') not in key_columns\n                ]\n            )\n            on_conditions = \" AND \".join(\n                [f'target.\"{col.upper()}\" = source.\"{col.upper()}\"' for col in key_columns]\n            )\n            sql = f\"\"\"\n                MERGE INTO {table_name} target\n                USING (SELECT {insert_values} FROM dual) source ({insert_cols})\n                ON ({on_conditions})\n                WHEN MATCHED THEN\n                    UPDATE SET {update_set}\n                WHEN NOT MATCHED THEN\n                    INSERT ({insert_cols}) VALUES ({insert_values})\n            \"\"\"\n            cursor = conn.cursor()\n            timing.append(time.perf_counter())\n            cursor.executemany(sql, data)\n            conn.commit()\n            timing.append(time.perf_counter())\n            self.logger.info(\n                f\"Upserted {cursor.rowcount} rows into {table_name} in \\\n                    {timing[-1] - timing[0]:.2f} seconds\"\n            )\n            self.logger.info(f\"Executed SQL: {sql}\")\n\n    def close(self):\n        \"\"\"Close the connection pool and clean up resources.\n\n        Raises:\n            DatabaseError: If closing the pool fails.\n        \"\"\"\n        self.logger.info(\"Closing OracleDBLoader and its connection pool\")\n        try:\n            pool = odb.get_pool(self.pool_alias)\n            if isinstance(pool, odb.ConnectionPool):\n                pool.close()\n            self.logger.info(\"Connection pool closed successfully\")\n        except odb.DatabaseError as e:\n            self.logger.error(f\"Failed to close connection pool: {e}\")\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.__init__","title":"<code>__init__(dsn=None, user=None, password=None, min_pool=1, max_pool=1, increment_pool=0, timeout=60)</code>","text":"<p>Initialize OracleDBLoader with connection pool.</p> <p>Parameters:</p> Name Type Description Default <code>dsn</code> <code>Optional[str]</code> <p>Oracle database service name. Defaults to env variable \"ORACLE_DSN\".</p> <code>None</code> <code>user</code> <code>Optional[str]</code> <p>Database username. Defaults to env variable \"ORACLE_USR\".</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Database password. Defaults to env variable \"ORACLE_PWD\".</p> <code>None</code> <code>min_pool</code> <code>int</code> <p>Minimum pool size. Defaults to 1.</p> <code>1</code> <code>max_pool</code> <code>int</code> <p>Maximum pool size. Defaults to 1.</p> <code>1</code> <code>increment_pool</code> <code>int</code> <p>Pool increment size. Defaults to 0.</p> <code>0</code> <code>timeout</code> <code>int</code> <p>Connection timeout in seconds. Defaults to 60.</p> <code>60</code> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If connection pool creation fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def __init__(\n    self,\n    dsn: Optional[str] = None,\n    user: Optional[str] = None,\n    password: Optional[str] = None,\n    min_pool: int = 1,\n    max_pool: int = 1,\n    increment_pool: int = 0,\n    timeout: int = 60,\n):\n    \"\"\"Initialize OracleDBLoader with connection pool.\n\n    Args:\n        dsn: Oracle database service name. Defaults to env variable \"ORACLE_DSN\".\n        user: Database username. Defaults to env variable \"ORACLE_USR\".\n        password: Database password. Defaults to env variable \"ORACLE_PWD\".\n        min_pool: Minimum pool size. Defaults to 1.\n        max_pool: Maximum pool size. Defaults to 1.\n        increment_pool: Pool increment size. Defaults to 0.\n        timeout: Connection timeout in seconds. Defaults to 60.\n\n    Raises:\n        DatabaseError: If connection pool creation fails.\n    \"\"\"\n    super().__init__()\n    self.dsn = dsn or os.getenv(\"ORACLE_DSN\")\n    self.user = user or os.getenv(\"ORACLE_USR\")\n    self.password = password or os.getenv(\"ORACLE_PWD\")\n    self.pool_alias = utils.id_generator()\n    odb.defaults.config_dir = (\n        \"/Users/astropd/Projects/oracle/oracle-19c/oradata/dbconfig/D7WOB1D1\"\n    )\n    odb.create_pool(\n        user=self.user,\n        password=self.password,\n        dsn=self.dsn,\n        min=min_pool,\n        max=max_pool,\n        increment=increment_pool,\n        pool_alias=self.pool_alias,\n        timeout=timeout,\n    )\n    self.test_connection()\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.check_table_exists","title":"<code>check_table_exists(table_name)</code>","text":"<p>Check if a table exists in the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the table exists, False otherwise.</p> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If the check fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def check_table_exists(self, table_name: str) -&gt; bool:\n    \"\"\"Check if a table exists in the database.\n\n    Args:\n        table_name: Name of the table to check.\n\n    Returns:\n        True if the table exists, False otherwise.\n\n    Raises:\n        DatabaseError: If the check fails.\n    \"\"\"\n    self.logger.info(f\"Checking if table {table_name} exists\")\n    exists = False\n    with odb.connect(pool_alias=self.pool_alias) as conn:\n        cursor = conn.cursor()\n        cursor.execute(\n            \"SELECT COUNT(*) FROM all_tables WHERE table_name \\\n                = :tbl_name AND owner = :owner\",\n            tbl_name=table_name.upper(),\n            owner=self.user.upper(),\n        )\n        count = cursor.fetchone()[0]\n        exists = count &gt; 0\n        self.logger.info(f\"Table {table_name} exists: {exists}\")\n    return exists\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.close","title":"<code>close()</code>","text":"<p>Close the connection pool and clean up resources.</p> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If closing the pool fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def close(self):\n    \"\"\"Close the connection pool and clean up resources.\n\n    Raises:\n        DatabaseError: If closing the pool fails.\n    \"\"\"\n    self.logger.info(\"Closing OracleDBLoader and its connection pool\")\n    try:\n        pool = odb.get_pool(self.pool_alias)\n        if isinstance(pool, odb.ConnectionPool):\n            pool.close()\n        self.logger.info(\"Connection pool closed successfully\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Failed to close connection pool: {e}\")\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.count_rows","title":"<code>count_rows(table_name)</code>","text":"<p>Get the number of rows in a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of rows in the table.</p> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If the query fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def count_rows(self, table_name: str) -&gt; int:\n    \"\"\"Get the number of rows in a table.\n\n    Args:\n        table_name: Name of the table.\n\n    Returns:\n        The number of rows in the table.\n\n    Raises:\n        DatabaseError: If the query fails.\n    \"\"\"\n    self.logger.info(f\"Getting row count for table {table_name}\")\n    row_count = 0\n    try:\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cursor: odb.Cursor = conn.cursor()\n            cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n            row_count: int = cursor.fetchone()[0]\n            self.logger.info(f\"Table {table_name} has {row_count} rows\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Row count retrieval failed: {e}\")\n        raise\n    return row_count\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.create_table","title":"<code>create_table(table_name, table)</code>","text":"<p>Create a new table in the database based on DataFrame structure.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to create.</p> required <code>df</code> <p>DataFrame whose structure defines the table columns and types.</p> required <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If table creation fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def create_table(self, table_name: str, table: pa.Table) -&gt; None:\n    \"\"\"Create a new table in the database based on DataFrame structure.\n\n    Args:\n        table_name: Name of the table to create.\n        df: DataFrame whose structure defines the table columns and types.\n\n    Raises:\n        DatabaseError: If table creation fails.\n    \"\"\"\n    ddl: str = self.generate_oracle_ddl(table, table_name)\n    self.logger.info(f\"Generated DDL: {ddl}\")\n    try:\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cursor: odb.Cursor = conn.cursor()\n            cursor.execute(ddl)\n            conn.commit()\n            self.logger.info(f\"Table {table_name} created successfully\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Creating table failed: {e}\")\n        raise\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.drop_table","title":"<code>drop_table(table_name)</code>","text":"<p>Drop (delete) a table from the database.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to drop.</p> required <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If the drop operation fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def drop_table(self, table_name: str) -&gt; None:\n    \"\"\"Drop (delete) a table from the database.\n\n    Args:\n        table_name: Name of the table to drop.\n\n    Raises:\n        DatabaseError: If the drop operation fails.\n    \"\"\"\n    self.logger.info(f\"Dropping table {table_name}\")\n    try:\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cursor: odb.Cursor = conn.cursor()\n            cursor.execute(f\"DROP TABLE {table_name} PURGE\")\n            conn.commit()\n            self.logger.info(f\"Table {table_name} dropped successfully\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Dropping table failed: {e}\")\n        raise\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.execute_query","title":"<code>execute_query(query)</code>","text":"<p>Execute a SQL query and return the results as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>An Oracle DataFrame containing the query results.</p> <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If the query fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def execute_query(self, query: str) -&gt; odb.DataFrame:\n    \"\"\"Execute a SQL query and return the results as a DataFrame.\n\n    Args:\n        query: The SQL query to execute.\n\n    Returns:\n        An Oracle DataFrame containing the query results.\n\n    Raises:\n        DatabaseError: If the query fails.\n    \"\"\"\n    self.logger.info(f\"Executing query: {query}\")\n    with odb.connect(pool_alias=self.pool_alias) as conn:\n        df: odb.DataFrame = conn.fetch_df_all(query)\n        # self.logger.info(f\"Query executed successfully with shape {df.shape}\")\n        self.logger.info(f\"Query returned {df.num_rows()} rows\")\n        return df\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.generate_oracle_ddl","title":"<code>generate_oracle_ddl(table, table_name)</code>","text":"<p>Generate Oracle DDL CREATE TABLE statement from a pyarrow Table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>pyarrow Table whose schema defines the table columns.</p> required <code>table_name</code> <code>str</code> <p>Name of the table to create.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A SQL CREATE TABLE statement string.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def generate_oracle_ddl(self, table: pa.Table, table_name: str) -&gt; str:\n    \"\"\"Generate Oracle DDL CREATE TABLE statement from a pyarrow Table.\n\n    Args:\n        table: pyarrow Table whose schema defines the table columns.\n        table_name: Name of the table to create.\n\n    Returns:\n        A SQL CREATE TABLE statement string.\n    \"\"\"\n    column_defs = []\n\n    def _field_to_oracle_type(field: pa.Field, column=None):\n        t: pa.DataType = field.type\n        # Decimal -&gt; NUMBER(precision,scale)\n        if pa.types.is_decimal(t):\n            try:\n                return f\"NUMBER({t.precision},{t.scale})\"\n            except Exception:\n                return \"NUMBER\"\n\n        # Integers -&gt; NUMBER\n        if pa.types.is_integer(t):\n            return \"NUMBER\"\n\n        # Floats -&gt; BINARY_DOUBLE / BINARY_FLOAT\n        if pa.types.is_float64(t):\n            return \"BINARY_DOUBLE\"\n        if pa.types.is_float32(t):\n            return \"BINARY_FLOAT\"\n\n        # Booleans -&gt; NUMBER(1)\n        if pa.types.is_boolean(t):\n            return \"NUMBER(1)\"\n\n        # Binary types -&gt; BLOB\n        if pa.types.is_large_binary(t) or pa.types.is_binary(t):\n            return \"BLOB\"\n\n        # Timestamp -&gt; TIMESTAMP (with TZ if present)\n        if pa.types.is_timestamp(t):\n            tz = getattr(t, \"tz\", None)\n            if tz:\n                return \"TIMESTAMP WITH TIME ZONE\"\n            return \"TIMESTAMP\"\n\n        # Strings -&gt; VARCHAR2 with a safe length or CLOB for large values\n        if pa.types.is_large_string(t):\n            return \"CLOB\"\n        if pa.types.is_string(t):\n            # try to infer max length from the column values if provided\n            max_len = 0\n            if column is not None:\n                try:\n                    max_len: int = max(\n                        (len(x) for x in column.to_pylist() if x is not None), default=0\n                    )\n                except Exception:\n                    max_len = 0\n            # conservative byte multiplier for multibyte characters\n            length = max(255, int(max_len * 3))\n            if length &lt;= 4000:\n                return f\"VARCHAR2({min(length, 4000)})\"\n            return \"CLOB\"\n\n        # Lists or structs representing vectors -&gt; BLOB (DB_TYPE_VECTOR)\n        if pa.types.is_list(t) or pa.types.is_struct(t):\n            return \"BLOB\"\n\n        # Fallback to CLOB for other types\n        return \"CLOB\"\n\n    # Map each field in the schema to an Oracle type\n    for i, field in enumerate(table.schema):\n        col_name = field.name\n        # provide the column chunk for possible length inference\n        column = None\n        try:\n            column = table.column(i)\n        except Exception:\n            column = None\n\n        oracle_type = _field_to_oracle_type(field, column)\n        column_defs.append(f'    \"{col_name.upper()}\" {oracle_type}')\n\n    ddl = f'CREATE TABLE \"{table_name.upper()}\" (\\n' + \",\\n\".join(column_defs) + \"\\n)\"\n    return ddl\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.insert_data","title":"<code>insert_data(data, table_name)</code>","text":"<p>Insert pyarrow Table data into an Oracle table using parameterized queries.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table</code> <p>pyarrow Table containing the data to insert.</p> required <code>table_name</code> <code>str</code> <p>Name of the target table.</p> required Source code in <code>src/etl/loaders.py</code> <pre><code>def insert_data(self, data: pa.Table, table_name: str) -&gt; None:\n    \"\"\"Insert pyarrow Table data into an Oracle table using parameterized queries.\n\n    Args:\n        data: pyarrow Table containing the data to insert.\n        table_name: Name of the target table.\n    \"\"\"\n    self.logger.info(f\"Inserting data into table {table_name}\")\n    timing = []\n    with odb.connect(pool_alias=self.pool_alias) as conn:\n        cols = \", \".join([f'\"{c.upper()}\"' for c in data.column_names])\n        placeholders = \",\".join([f\":{i + 1}\" for i in range(len(data.column_names))])\n        sql = f\"INSERT INTO {table_name} ({cols}) VALUES ({placeholders})\"\n        cursor = conn.cursor()\n        timing.append(time.perf_counter())\n        cursor.executemany(sql, data)\n        conn.commit()\n        timing.append(time.perf_counter())\n        self.logger.info(\n            f\"Inserted {cursor.rowcount} rows into {table_name} in \\\n                {timing[-1] - timing[0]:.2f} seconds\"\n        )\n        self.logger.info(f\"Executed SQL: {sql}\")\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.insert_full_data","title":"<code>insert_full_data(table, table_name)</code>","text":"<p>Insert DataFrame data using Oracle direct path load (high performance).</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>pyarrow Table containing the data to insert.</p> required <code>table_name</code> <code>str</code> <p>Name of the target table.</p> required Source code in <code>src/etl/loaders.py</code> <pre><code>def insert_full_data(self, table: pa.Table, table_name: str) -&gt; None:\n    \"\"\"Insert DataFrame data using Oracle direct path load (high performance).\n\n    Args:\n        table: pyarrow Table containing the data to insert.\n        table_name: Name of the target table.\n    \"\"\"\n    self.logger.info(f\"Inserting full data into table {table_name}\")\n    timing = []\n    with odb.connect(pool_alias=self.pool_alias) as conn:\n        timing.append(time.perf_counter())\n        conn.direct_path_load(\n            schema_name=self.user,\n            table_name=table_name,\n            column_names=table.column_names,\n            data=table,\n        )\n        conn.commit()\n        timing.append(time.perf_counter())\n        self.logger.info(\n            f\"Full data inserted into {table_name} in {timing[-1] - timing[0]:.2f} seconds\"\n        )\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.test_connection","title":"<code>test_connection()</code>","text":"<p>Test the connection to the Oracle database.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if connection is healthy, False otherwise.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def test_connection(self) -&gt; bool:\n    \"\"\"Test the connection to the Oracle database.\n\n    Returns:\n        True if connection is healthy, False otherwise.\n    \"\"\"\n    self.logger.info(f\"Testing connection to Oracle DB at {self.dsn}\")\n    healthy = False\n    try:\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            healthy = conn.is_healthy()\n            self.logger.info(f\"Connection healthy: {healthy}\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Connection failed: {e}\")\n        healthy = False\n    return healthy\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.truncate_table","title":"<code>truncate_table(table_name)</code>","text":"<p>Truncate (remove all data from) an Oracle table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Name of the table to truncate.</p> required <p>Raises:</p> Type Description <code>DatabaseError</code> <p>If truncation fails.</p> Source code in <code>src/etl/loaders.py</code> <pre><code>def truncate_table(self, table_name: str) -&gt; None:\n    \"\"\"Truncate (remove all data from) an Oracle table.\n\n    Args:\n        table_name: Name of the table to truncate.\n\n    Raises:\n        DatabaseError: If truncation fails.\n    \"\"\"\n    self.logger.info(f\"Truncating table {table_name}\")\n    try:\n        with odb.connect(pool_alias=self.pool_alias) as conn:\n            cursor = conn.cursor()\n            cursor.execute(f\"TRUNCATE TABLE {table_name}\")\n            conn.commit()\n            self.logger.info(f\"Table {table_name} truncated successfully\")\n    except odb.DatabaseError as e:\n        self.logger.error(f\"Truncation failed: {e}\")\n        raise\n</code></pre>"},{"location":"src/etl/loaders/#etl.loaders.OracleDBLoader.upsert_data","title":"<code>upsert_data(data, table_name, key_columns)</code>","text":"<p>Upsert pyarrow Table data into an Oracle table using MERGE statement.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Table</code> <p>pyarrow Table containing the data to upsert.</p> required <code>table_name</code> <code>str</code> <p>Name of the target table.</p> required <code>key_columns</code> <code>list[str]</code> <p>List of column names that form the primary key for upsert.</p> required Source code in <code>src/etl/loaders.py</code> <pre><code>def upsert_data(self, data: pa.Table, table_name: str, key_columns: list[str]) -&gt; None:\n    \"\"\"Upsert pyarrow Table data into an Oracle table using MERGE statement.\n\n    Args:\n        data: pyarrow Table containing the data to upsert.\n        table_name: Name of the target table.\n        key_columns: List of column names that form the primary key for upsert.\n    \"\"\"\n    self.logger.info(f\"Upserting data into table {table_name}\")\n    timing = []\n    with odb.connect(pool_alias=self.pool_alias) as conn:\n        cols = [f'\"{c.upper()}\"' for c in data.column_names]\n        insert_cols = \", \".join(cols)\n        insert_values = \",\".join([f\":{i + 1}\" for i in range(len(data.column_names))])\n        update_set = \", \".join(\n            [\n                f\"{col} = :{i + 1}\"\n                for i, col in enumerate(cols)\n                if col.strip('\"') not in key_columns\n            ]\n        )\n        on_conditions = \" AND \".join(\n            [f'target.\"{col.upper()}\" = source.\"{col.upper()}\"' for col in key_columns]\n        )\n        sql = f\"\"\"\n            MERGE INTO {table_name} target\n            USING (SELECT {insert_values} FROM dual) source ({insert_cols})\n            ON ({on_conditions})\n            WHEN MATCHED THEN\n                UPDATE SET {update_set}\n            WHEN NOT MATCHED THEN\n                INSERT ({insert_cols}) VALUES ({insert_values})\n        \"\"\"\n        cursor = conn.cursor()\n        timing.append(time.perf_counter())\n        cursor.executemany(sql, data)\n        conn.commit()\n        timing.append(time.perf_counter())\n        self.logger.info(\n            f\"Upserted {cursor.rowcount} rows into {table_name} in \\\n                {timing[-1] - timing[0]:.2f} seconds\"\n        )\n        self.logger.info(f\"Executed SQL: {sql}\")\n</code></pre>"},{"location":"src/etl/pipeline/","title":"Pipeline Module","text":"<p>The pipeline orchestrates the extract\u2011transform\u2011load process.</p>"},{"location":"src/etl/pipeline/#function","title":"Function","text":"<ul> <li><code>run_pipeline()</code> \u2013 Executes the full ETL flow: extracts data, cleans it, and loads it.</li> </ul>"},{"location":"src/etl/pipeline/#example","title":"Example","text":"<pre><code>from etl.pipeline import run_pipeline\nrun_pipeline()\n</code></pre>"},{"location":"src/etl/pipeline/#source","title":"Source","text":""},{"location":"src/etl/utils/","title":"Utils Module","text":"<p>Utility functions used across the ETL pipeline.</p>"},{"location":"src/etl/utils/#functions","title":"Functions","text":"<ul> <li><code>clean_df(df)</code> \u2013 Drops list/struct columns and casts Boolean columns to <code>Int8</code> for Oracle compatibility.</li> <li><code>id_generator()</code> \u2013 Generates unique request IDs.</li> </ul>"},{"location":"src/etl/utils/#example","title":"Example","text":"<pre><code>from etl.utils import clean_df, id_generator\nrequest_id = id_generator()\ncleaned = clean_df(df)\n</code></pre>"},{"location":"src/etl/utils/#source","title":"Source","text":""},{"location":"src/etl/utils/#etl.utils.clean_df","title":"<code>clean_df(df)</code>","text":"<p>Return a copy of <code>df</code> where all Boolean columns are converted to numeric (Int8) columns with 1 for True and 0 for False.</p> Source code in <code>src/etl/utils.py</code> <pre><code>def clean_df(df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"\n    Return a copy of ``df`` where all Boolean columns are converted to numeric\n    (Int8) columns with 1 for True and 0 for False.\n    \"\"\"\n\n    list_cols = [\n        col_name\n        for col_name, dtype in zip(df.columns, df.dtypes)\n        if isinstance(dtype, pl.List) or isinstance(dtype, pl.Struct)\n    ]\n    df = df.drop(list_cols)\n    # Identify Boolean columns\n    bool_cols = [c for c, t in df.schema.items() if t == pl.Boolean]\n\n    # If there are none, just return the original frame\n    if not bool_cols:\n        return df\n\n    # Convert each Boolean column to Int8 (1/0)\n    # ``cast(pl.Int8)`` works because Polars treats True/False as 1/0 internally.\n    conversions = [pl.col(col).cast(pl.Int8).alias(col) for col in bool_cols]\n\n    # Apply the conversions and return a new DataFrame\n    return df.with_columns(conversions)\n</code></pre>"},{"location":"src/etl/utils/#etl.utils.id_generator","title":"<code>id_generator(size=6, chars=string.ascii_uppercase)</code>","text":"<p>Generate a random string of specified length using given characters.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <p>Length of the generated string. Defaults to 6.</p> <code>6</code> <code>chars</code> <p>String of characters to choose from. Defaults to uppercase ASCII letters.</p> <code>ascii_uppercase</code> <p>Returns:</p> Type Description <p>A random string of the specified size.</p> Source code in <code>src/etl/utils.py</code> <pre><code>def id_generator(size=6, chars=string.ascii_uppercase):\n    \"\"\"Generate a random string of specified length using given characters.\n\n    Args:\n        size: Length of the generated string. Defaults to 6.\n        chars: String of characters to choose from. Defaults to uppercase\n            ASCII letters.\n\n    Returns:\n        A random string of the specified size.\n    \"\"\"\n    return \"\".join(random.choice(chars) for _ in range(size))\n</code></pre>"}]}